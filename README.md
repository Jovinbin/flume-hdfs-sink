# flume-hdfs-sink
flume HDFS Sink的高并发版本，并且带有数据漂移处理
flume HDFS Sink的基本处理逻辑是，HDFSEventSink去channel中抽取一批数据，然后利用BucketWriter将数据写到HDFS。原声flume入到HDFS的效率只有7M/s。所以对其进行个性化改造。
  由于，我们的数据是从Kafka抽取，flume使用的是memory channel。根据测试，前面两个环节效率非常高，大概是以网络传输速度为上限。很快就确定瓶颈是在HDFS Sink这部分。另外，在路径配置了日期过后，
每到一条数据，Sink都会判断时间是否越界，比如，你设置了带日期的路径，那么到今晚24点的时候，如果从24点到凌晨，当前文件并没有积累到配置大小（如512M），Sink还是会将文件切段，然后在第二天的路径
新建一个文件。这种机制对于小时，分钟也同样存在。该机制毫无疑问会增加Sink处理数据的负担。我们的需求比较偏向于，生成完整的文件，比如我们的block块是512，我们希望每个文件都是512M，而不是一对临
时文件。所以，最开始我去掉这一部分，数据处理速度达到了9M/s，但是这个效率远远达不到我们的需求。我尝试过单个flume多个Sink，但是也无明显效果。
  为了追求极致性能，所以决定拿瓶颈Sink开刀，最有效的方法就是增加并行度，经过阅读其源码过后，我决定拆分HDFSEventSink的process方法，将HDFSEventSink.process方法获取到的数据，分发到各个线
程的缓冲区，然后每个线程都拥有一个BucketWriter，每个线程从缓冲区抽取数据，然后落地到HDFS。拆分出来的处理逻辑，我放在了HDFSEventSink内部类parallelSink，并在flume open HDFSEventSink
的时候启动线程。经过测试，该想法是可行的，并且将数据入库效率直接提升到56M/s左右。数据分发逻辑见HDFSEventSink.process方法，BucketWriter的管理见HDFSEventSink$parallelSink.run。另外，
文件路径的确认不在HDFSEventSink，根据之前描述的需求，我将其下沉到创建BucketWriter.open方法。
  关于并发量的确认，当并发度超过，一定量的时候，增加并发毒并不能增加效率，反而还降低了效率，从20线程到30线程处理一亿条数据增加了50%还要多，相当于，如果10条线程处理一亿条数据如果需要10分钟，20
条线程处理一亿条数据不仅没有变成5分钟，反而增加到20分钟。明显有资源竞争存在，经过反复测试，基本可以确认是gzip的竞争，取消掉压缩，采用Text格式过后，不再像之前，20条线程处理一亿条数据耗时6分10秒
-6分30秒，入库效率最高92M/s。
